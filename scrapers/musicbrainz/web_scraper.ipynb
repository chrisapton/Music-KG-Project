{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import os\n",
    "import time  # Importing time for delay\n",
    "\n",
    "def get_artist_musicbrainz_url(artist_name):\n",
    "    \"\"\"Search for the artist on MusicBrainz and get the artist URL.\"\"\"\n",
    "    base_url = \"https://musicbrainz.org\"\n",
    "    search_url = f\"{base_url}/search?query={artist_name.replace(' ', '+')}&type=artist&method=indexed\"\n",
    "    try:\n",
    "        response = requests.get(search_url, headers={\"User-Agent\": \"YourAppName/1.0 (your-email@example.com)\"})\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find the first search result link\n",
    "        artist_link = soup.select_one(\"table.tbl a\")\n",
    "        if artist_link:\n",
    "            return base_url + artist_link['href']\n",
    "        return None\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error fetching artist page: {e}\")\n",
    "        return None\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Remove unnecessary characters and clean up text.\"\"\"\n",
    "    text = text.replace('\\n', ' ').replace('\\r', '').strip()\n",
    "    return \" \".join(text.split())\n",
    "\n",
    "def scrape_wikipedia_summary(artist_url):\n",
    "    \"\"\"Scrape the Wikipedia summary from the artist's MusicBrainz page.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(artist_url, headers={\"User-Agent\": \"YourAppName/1.0 (your-email@example.com)\"})\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # Adding a small delay after the request to ensure content has loaded\n",
    "        time.sleep(1)  # Wait for 1 second\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Find the Wikipedia section using the div with class \"wikipedia-extract-body\"\n",
    "        wiki_div = soup.find(\"div\", class_=\"wikipedia-extract-body wikipedia-extract-collapse\")\n",
    "        if wiki_div:\n",
    "            print(wiki_div)\n",
    "            # Extract all paragraphs separately and join them with proper spacing\n",
    "            paragraphs = wiki_div.find_all(\"p\")\n",
    "            summary_parts = []\n",
    "            for p in paragraphs:\n",
    "                # Extract the text from each paragraph\n",
    "                paragraph_text = p.get_text(separator=\" \", strip=True)\n",
    "                cleaned_text = clean_text(paragraph_text)\n",
    "                summary_parts.append(cleaned_text)\n",
    "\n",
    "            # Join all paragraphs into a single summary\n",
    "            full_summary = \" \".join(summary_parts)\n",
    "\n",
    "            # Remove any \"Continue reading at Wikipedia\" links or extra texts\n",
    "            if \"Continue reading at Wikipedia\" in full_summary:\n",
    "                full_summary = full_summary.split(\"Continue reading at Wikipedia\")[0].strip()\n",
    "\n",
    "            return full_summary\n",
    "        return \"N/A\"\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error scraping Wikipedia summary: {e}\")\n",
    "        return \"N/A\"\n",
    "\n",
    "def fetch_artist_details(artist_name):\n",
    "    \"\"\"Fetch the Wikipedia summary directly from the MusicBrainz artist page.\"\"\"\n",
    "    artist_url = get_artist_musicbrainz_url(artist_name)\n",
    "    if artist_url:\n",
    "        print(f\"Found artist page: {artist_url}\")\n",
    "        wikipedia_summary = scrape_wikipedia_summary(artist_url)\n",
    "        return {\"wikipedia_summary\": wikipedia_summary}\n",
    "    else:\n",
    "        print(\"Artist page not found.\")\n",
    "        return {\"wikipedia_summary\": \"N/A\"}\n",
    "\n",
    "def process_file(file_path):\n",
    "    \"\"\"Process each JSONL file to extract song and artist details.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                try:\n",
    "                    # Parse the JSON line\n",
    "                    song_data = json.loads(line.strip())\n",
    "\n",
    "                    # Handle single and multiple artist formats\n",
    "                    artists = song_data.get(\"artist\")\n",
    "                    if isinstance(artists, str):\n",
    "                        artists = [artists]\n",
    "\n",
    "                    song_name = song_data.get(\"title\", \"N/A\")\n",
    "\n",
    "                    for artist_name in artists:\n",
    "                        # Fetch the Wikipedia summary\n",
    "                        artist_details = fetch_artist_details(artist_name)\n",
    "                        wikipedia_summary = artist_details.get(\"wikipedia_summary\", \"N/A\")\n",
    "\n",
    "                        print(f\"Title: {song_name}\")\n",
    "                        print(f\"Artist: {artist_name}\")\n",
    "                        print(f\"Wikipedia Summary: {wikipedia_summary}\")\n",
    "                        print(\"-\" * 50)\n",
    "                        break\n",
    "                except json.JSONDecodeError:\n",
    "                    print(\"Error: Invalid JSON line.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{file_path}' not found.\")\n",
    "\n",
    "# Base directory for files\n",
    "base_dir = '/Users/chrisapton/Desktop/Spring 2025/DSCI 558/Music-KG-Project/data/raw_data/'\n",
    "\n",
    "# Loop through pages 1 to 10\n",
    "for page_number in range(1, 11):\n",
    "    file_name = f'whosampled_tracks_2024_{page_number}.jsonl'\n",
    "    file_path = os.path.join(base_dir, file_name)\n",
    "    print(f\"Processing file: {file_name}\")\n",
    "    process_file(file_path)\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: whosampled_tracks_2022.jsonl\n",
      "Resuming from line 6634\n",
      "https://musicbrainz.org/ws/2/recording/?query=recording%3A%22Planet%20Rock%20%28Ultimix%29%22%20AND%20%28artist%3A%22Afrika%20Bambaataa%22%20OR%20artist%3A%22Soulsonic%20Force%22%29&fmt=json\n",
      "Found artist URL: https://musicbrainz.org/artist/fe3503fb-146f-4d68-a591-a7e5798c321f (Attempt 1)\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "\n",
    "def get_artist_musicbrainz_url(artist_name):\n",
    "    \"\"\"Search for the artist on MusicBrainz and get the artist URL.\"\"\"\n",
    "    base_url = \"https://musicbrainz.org\"\n",
    "    search_url = f\"{base_url}/search?query={artist_name.replace(' ', '+')}&type=artist&method=indexed\"\n",
    "    attempts = 5  # Number of retries\n",
    "\n",
    "    for attempt in range(1, attempts + 1):\n",
    "        try:\n",
    "            response = requests.get(search_url, headers={\"User-Agent\": \"YourAppName/1.0 (your-email@example.com)\"})\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # Find the first search result link\n",
    "            artist_link = soup.select_one(\"table.tbl a\")\n",
    "            if artist_link:\n",
    "                full_url = base_url + artist_link['href']\n",
    "                print(f\"Found artist URL: {full_url} (Attempt {attempt})\")\n",
    "                return full_url\n",
    "            \n",
    "            print(f\"Attempt {attempt}: No artist link found. Retrying...\")\n",
    "            time.sleep(1)\n",
    "\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Attempt {attempt}: Error fetching artist page: {e}\")\n",
    "            time.sleep(1)  # Brief pause before retrying\n",
    "\n",
    "    # If all attempts fail, print the fetched page content for debugging\n",
    "    print(f\"Failed to retrieve artist URL after {attempts} attempts.\")\n",
    "    print(f\"Search URL: {search_url}\")\n",
    "    print(\"Page content:\")\n",
    "    print(response.text[:1000])  # Print the first 1000 characters to avoid overwhelming output\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_release_date(recording):\n",
    "    \"\"\"Extract the release date from a recording.\"\"\"\n",
    "    # Check for \"first-release-date\" field first\n",
    "    if 'first-release-date' in recording:\n",
    "        return recording['first-release-date']\n",
    "\n",
    "    # Fallback: Check inside the \"releases\" list\n",
    "    release_dates = []\n",
    "    if 'releases' in recording and recording['releases']:\n",
    "        for release in recording['releases']:\n",
    "            date = release.get('date', 'N/A')\n",
    "            if date != 'N/A':\n",
    "                release_dates.append(date)\n",
    "\n",
    "        # Sort by length and value to prioritize full dates (YYYY-MM-DD)\n",
    "        if release_dates:\n",
    "            return min(release_dates, key=lambda x: (len(x), x))\n",
    "\n",
    "    return \"N/A\"\n",
    "\n",
    "\n",
    "def get_genres(recording):\n",
    "    \"\"\"Extract the genres from a recording.\"\"\"\n",
    "    print(recording)\n",
    "    genres = set()\n",
    "\n",
    "    # Check for genres directly in the recording tags\n",
    "    if 'tags' in recording:\n",
    "        for tag in recording['tags']:\n",
    "            genres.add(tag['name'])\n",
    "\n",
    "    # Fallback: Check the release group for genres\n",
    "    if 'releases' in recording and recording['releases']:\n",
    "        for release in recording['releases']:\n",
    "            if 'release-group' in release and 'tags' in release['release-group']:\n",
    "                for tag in release['release-group']['tags']:\n",
    "                    genres.add(tag['name'])\n",
    "\n",
    "    # If no genres found, try to check the artist tags\n",
    "    if not genres and 'artist-credit' in recording:\n",
    "        for artist_credit in recording['artist-credit']:\n",
    "            artist = artist_credit.get('artist', {})\n",
    "            if 'tags' in artist:\n",
    "                for tag in artist['tags']:\n",
    "                    genres.add(tag['name'])\n",
    "\n",
    "    return list(genres)\n",
    "\n",
    "\n",
    "def scrape_wikipedia_summary_selenium(artist_url):\n",
    "    \"\"\"Scrape the Wikipedia summary from the artist's MusicBrainz page using Selenium.\"\"\"\n",
    "    try:\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument('--headless')\n",
    "        chrome_options.add_argument('--no-sandbox')\n",
    "        chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "\n",
    "        # Path to your ChromeDriver\n",
    "        driver_path = '/Users/chrisapton/Desktop/Spring 2025/DSCI 558/chromedriver-mac-arm64/chromedriver'\n",
    "        service = Service(driver_path)\n",
    "        driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "        driver.get(artist_url)\n",
    "\n",
    "        try:\n",
    "            wiki_div = WebDriverWait(driver, 5).until(\n",
    "                EC.presence_of_element_located((By.CLASS_NAME, 'wikipedia-extract-body'))\n",
    "            )\n",
    "            summary = wiki_div.text.strip()\n",
    "\n",
    "            # Remove any \"Continue reading at Wikipedia\" text\n",
    "            if \"Continue reading at Wikipedia\" in summary:\n",
    "                summary = summary.split(\"Continue reading at Wikipedia\")[0].strip()\n",
    "\n",
    "            driver.quit()\n",
    "            return summary if summary else \"N/A\"\n",
    "        except Exception as e:\n",
    "            print(artist_url)\n",
    "            print(f\"Error finding Wikipedia summary: {e}\")\n",
    "\n",
    "            driver.quit()\n",
    "            return \"N/A\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error with Selenium: {e}\")\n",
    "        return \"N/A\"\n",
    "\n",
    "def fetch_song_details(song_name, artists):\n",
    "    \"\"\"Fetch song release date and genre from MusicBrainz.\"\"\"\n",
    "\n",
    "    if not artists:\n",
    "        print(f\"No artists found for song: {song_name}. Skipping...\")\n",
    "        return {\n",
    "            \"title\": song_name,\n",
    "            \"artist\": \"N/A\",\n",
    "            \"release_date\": \"N/A\",\n",
    "            \"genres\": [],\n",
    "            \"wikipedia_summary\": \"N/A\"\n",
    "        }\n",
    "    # Join artist names with \" AND \" for the query\n",
    "    artist_query = \" OR \".join([f'artist:\"{artist}\"' for artist in artists])\n",
    "    # Combine the title and artist queries\n",
    "    query = f'recording:\"{song_name}\" AND ({artist_query})'\n",
    "    # Encode the query to make it URL-safe\n",
    "    encoded_query = requests.utils.quote(query)\n",
    "    # Construct the full URL\n",
    "    url = f'https://musicbrainz.org/ws/2/recording/?query={encoded_query}&fmt=json'\n",
    "    print(url)\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers={\"User-Agent\": \"YourAppName/1.0 (your-email@example.com)\"})\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "\n",
    "        if 'recordings' in data and data['recordings']:\n",
    "            for recording in data['recordings']:\n",
    "                title = recording.get('title', 'N/A')\n",
    "                artist_credit = recording.get('artist-credit', [])\n",
    "\n",
    "                # Extract all artist names from the recording\n",
    "                recording_artists = [credit.get('name', 'N/A').lower() for credit in artist_credit]\n",
    "\n",
    "                # Check if all given artists are present in the recording\n",
    "                if all(artist.lower() in recording_artists for artist in artists) and song_name.lower() in title.lower():\n",
    "                    release_date = get_release_date(recording)\n",
    "                    genres = get_genres(recording)\n",
    "                    return {\n",
    "                        \"title\": title,\n",
    "                        \"artist\": [artist['name'] for artist in artist_credit],\n",
    "                        \"release_date\": release_date,\n",
    "                        \"genres\": genres\n",
    "                    }\n",
    "                else: \n",
    "                    print(\"can't find all artists: approx search\")\n",
    "                    release_date = get_release_date(recording)\n",
    "                    genres = get_genres(recording)\n",
    "                    return {\n",
    "                        \"title\": title,\n",
    "                        \"artist\": [artist['name'] for artist in artist_credit],\n",
    "                        \"release_date\": release_date,\n",
    "                        \"genres\": genres\n",
    "                    }\n",
    "        return {\"title\": song_name, \"artist\": \", \".join(artists), \"release_date\": \"N/A\", \"genres\": \"N/A\"}\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error fetching song details: {e}\")\n",
    "        return {\"title\": song_name, \"artist\": \", \".join(artists), \"release_date\": \"N/A\", \"genres\": \"N/A\"}\n",
    "\n",
    "def fetch_artist_wikipedia(artist_name):\n",
    "    \"\"\"Fetch Wikipedia summary for a given artist.\"\"\"\n",
    "    artist_url = get_artist_musicbrainz_url(artist_name)\n",
    "    wikipedia_summary = scrape_wikipedia_summary_selenium(artist_url)\n",
    "    return wikipedia_summary\n",
    "\n",
    "def fetch_artist_genres(artist_name):\n",
    "    \"\"\"\n",
    "    Fetches the top genres of an artist from the MusicBrainz API.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Step 1: Search for the artist to get their MusicBrainz ID\n",
    "        search_url = f'https://musicbrainz.org/ws/2/artist/?query={artist_name}&fmt=json'\n",
    "        search_response = requests.get(search_url, headers={\"User-Agent\": \"YourAppName/1.0 (your-email@example.com)\"})\n",
    "        search_response.raise_for_status()\n",
    "        search_data = search_response.json()\n",
    "        \n",
    "        # Get the first matching artist ID\n",
    "        if search_data['artists']:\n",
    "            artist_id = search_data['artists'][0]['id']\n",
    "        else:\n",
    "            print(f\"No artist found for {artist_name}\")\n",
    "            return []\n",
    "\n",
    "        # Step 2: Get the artist details using the ID\n",
    "        artist_url = f'https://musicbrainz.org/ws/2/artist/{artist_id}?inc=tags&fmt=json'\n",
    "        response = requests.get(artist_url, headers={\"User-Agent\": \"YourAppName/1.0 (your-email@example.com)\"})\n",
    "        response.raise_for_status()\n",
    "        artist_data = response.json()\n",
    "\n",
    "        # Extract genres from the tags field\n",
    "        genres = [tag['name'] for tag in artist_data.get('tags', [])]\n",
    "\n",
    "        # Return top 3-4 genres if available\n",
    "        return genres[:4] if genres else []\n",
    "    \n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error fetching artist genres: {e}\")\n",
    "        return []\n",
    "\n",
    "def process_file(file_path, page_number):\n",
    "    \"\"\"Process each JSONL file to extract song and artist details.\"\"\"\n",
    "    output_file = f'musicbrainz_tracks_{page_number}.jsonl'\n",
    "\n",
    "    # Get the number of lines already processed in the output file\n",
    "    processed_count = 0\n",
    "    try:\n",
    "        with open(output_file, 'r', encoding='utf-8') as file:\n",
    "            processed_count = sum(1 for _ in file)\n",
    "    except FileNotFoundError:\n",
    "        pass  # If the file doesn't exist, start from scratch\n",
    "\n",
    "    print(f\"Resuming from line {processed_count + 1}\")\n",
    "\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            for line_number, line in enumerate(file, start=1):\n",
    "                # Skip already processed lines\n",
    "                if line_number <= processed_count:\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    song_data = json.loads(line.strip())\n",
    "                    artists = song_data.get(\"artist\")\n",
    "                    if isinstance(artists, str):\n",
    "                        artists = [artists]\n",
    "\n",
    "                    song_name = song_data.get(\"title\", \"N/A\")\n",
    "\n",
    "                    # Fetch song details\n",
    "                    artist_details = fetch_song_details(song_name, artists)\n",
    "                    wikipedia_summaries = list(map(fetch_artist_wikipedia, artists))\n",
    "\n",
    "                    # Fetch genres if none found initially\n",
    "                    if len(artist_details['genres']) == 0:\n",
    "                        for artist in artists:\n",
    "                            artist_details['genres'].extend(fetch_artist_genres(artist))\n",
    "                        artist_details['genres'] = list(set(artist_details['genres']))\n",
    "\n",
    "                    # Print extracted details\n",
    "                    print(f\"ID: {song_data.get('whosampled_id', 'N/A')}\")\n",
    "                    print(f\"Title: {artist_details['title']}\")\n",
    "                    print(f\"Artist: {artist_details['artist']}\")\n",
    "                    print(f\"Release Date: {artist_details['release_date']}\")\n",
    "                    print(f\"Genres: {artist_details['genres']}\")\n",
    "                    print(f\"Wikipedia Summary: {wikipedia_summaries}\")\n",
    "                    print(\"-\" * 50)\n",
    "\n",
    "                    # Create the song info dictionary\n",
    "                    song_info = {\n",
    "                        \"id\": song_data.get(\"whosampled_id\", \"N/A\"),\n",
    "                        \"title\": artist_details.get(\"title\", \"N/A\"),\n",
    "                        \"artist\": artist_details.get(\"artist\", \"N/A\"),\n",
    "                        \"release_date\": artist_details.get(\"release_date\", \"N/A\"),\n",
    "                        \"genres\": artist_details.get(\"genres\", \"N/A\"),\n",
    "                        \"wikipedia_summary\": wikipedia_summaries,\n",
    "                    }\n",
    "\n",
    "                    # Append the song info to the output file\n",
    "                    with open(output_file, 'a', encoding='utf-8') as outfile:\n",
    "                        outfile.write(json.dumps(song_info) + '\\n')\n",
    "\n",
    "                except (json.JSONDecodeError, KeyError) as e:\n",
    "                    print(f\"Error processing line {line_number}: {e}\")\n",
    "                    song_info = {\n",
    "                        \"id\": song_data.get(\"whosampled_id\", \"N/A\"),\n",
    "                        \"title\": artist_details.get(\"title\", \"N/A\"),\n",
    "                        \"artist\": artist_details.get(\"artist\", \"N/A\"),\n",
    "                        \"release_date\": artist_details.get(\"release_date\", \"N/A\"),\n",
    "                        \"genres\": artist_details.get(\"genres\", \"N/A\"),\n",
    "                        \"wikipedia_summary\": [\"N/A\"],\n",
    "                    }\n",
    "\n",
    "                    # Append the song info to the output file\n",
    "                    with open(output_file, 'a', encoding='utf-8') as outfile:\n",
    "                        outfile.write(json.dumps(song_info) + '\\n')\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Unexpected error at line {line_number}: {e}\")\n",
    "                    song_info = {\n",
    "                        \"id\": song_data.get(\"whosampled_id\", \"N/A\"),\n",
    "                        \"title\": artist_details.get(\"title\", \"N/A\"),\n",
    "                        \"artist\": artist_details.get(\"artist\", \"N/A\"),\n",
    "                        \"release_date\": artist_details.get(\"release_date\", \"N/A\"),\n",
    "                        \"genres\": artist_details.get(\"genres\", \"N/A\"),\n",
    "                        \"wikipedia_summary\": [\"N/A\"],\n",
    "                    }\n",
    "\n",
    "                    # Append the song info to the output file\n",
    "                    with open(output_file, 'a', encoding='utf-8') as outfile:\n",
    "                        outfile.write(json.dumps(song_info) + '\\n')\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{file_path}' not found.\")       \n",
    "\n",
    "# Base directory for files\n",
    "base_dir = '/Users/chrisapton/Desktop/Spring 2025/DSCI 558/Music-KG-Project/data/processed/'\n",
    "\n",
    "# Loop through pages 1 to 10\n",
    "for page_number in range(2022, 2025):\n",
    "    file_name = f'whosampled_tracks_{page_number}.jsonl'\n",
    "    file_path = os.path.join(base_dir, file_name)\n",
    "    print(f\"Processing file: {file_name}\")\n",
    "    process_file(file_path, page_number)\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
